<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>ANN神经网络 - Even - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="LYR" /><meta name="description" content="人工神经网络(ANN) 人工神经网路在仿生上模仿(极其简单的模仿)了人脑的神经结构, 可以Learn from data. 和生成模型不同 ,神经网络不会去计算概率分" /><meta name="keywords" content="LYR的文档站, LYR的个人博客, even" />






<meta name="generator" content="Hugo 0.86.0 with theme even" />


<link rel="canonical" href="http://doc.lyr-2000.xyz/post/10.%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ann/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="ANN神经网络" />
<meta property="og:description" content="人工神经网络(ANN) 人工神经网路在仿生上模仿(极其简单的模仿)了人脑的神经结构, 可以Learn from data. 和生成模型不同 ,神经网络不会去计算概率分" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://doc.lyr-2000.xyz/post/10.%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ann/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-08-14T22:43:02+08:00" />
<meta property="article:modified_time" content="2021-08-14T22:43:02+08:00" />

<meta itemprop="name" content="ANN神经网络">
<meta itemprop="description" content="人工神经网络(ANN) 人工神经网路在仿生上模仿(极其简单的模仿)了人脑的神经结构, 可以Learn from data. 和生成模型不同 ,神经网络不会去计算概率分"><meta itemprop="datePublished" content="2021-08-14T22:43:02+08:00" />
<meta itemprop="dateModified" content="2021-08-14T22:43:02+08:00" />
<meta itemprop="wordCount" content="5221">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ANN神经网络"/>
<meta name="twitter:description" content="人工神经网络(ANN) 人工神经网路在仿生上模仿(极其简单的模仿)了人脑的神经结构, 可以Learn from data. 和生成模型不同 ,神经网络不会去计算概率分"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">LYR的文档站</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">首页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档页</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/friends">
        <li class="mobile-menu-item">大佬</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">LYR的文档站</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">首页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/friends">大佬</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">ANN神经网络</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-08-14 </span>
        
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#人工神经网络ann">人工神经网络(ANN)</a>
          <ul>
            <li><a href="#线性的ann结构">线性的ANN结构</a></li>
            <li><a href="#非线性ann结构">非线性ANN结构</a></li>
            <li><a href="#非线性annmlp的训练">非线性ANN(MLP)的训练</a></li>
            <li><a href="#一些关键证明">一些关键证明</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="人工神经网络ann">人工神经网络(ANN)</h2>
<p>人工神经网路在仿生上模仿(<strong>极其简单的模仿</strong>)了人脑的神经结构, 可以<strong>Learn from data</strong>. 和<strong>生成模型不同</strong> ,神经网络不会去计算概率分布,而是构建一个<strong>最优化的数学拟合表达式[1]</strong>. 去解决一些实际的问题(最广泛使用的还是<strong>分类问题</strong>).</p>
<p>如图为一个三层神经网络的基本结构, 第一层为输入层(input layer), 第二层为隐藏层(hidden layer), 第三层为输出层(output layer). 中间的节点为神经元(neuron)</p>
<img src='http://ufldl.stanford.edu/tutorial/images/Network331.png' style="width:400px">
<center>一个三层神经网络的基本结构
<p>神经网络的主要形式分为以下两种:</p>
<ul>
<li>
<p>基于监督(Supervised)学习的神经网络</p>
<ul>
<li><strong>分类问题(主要)</strong></li>
<li>回归问题[2]</li>
</ul>
</li>
<li>
<p>基于非监督(Unsupervised)学习的神经网络</p>
<ul>
<li>聚类</li>
<li>密度估计</li>
</ul>
</li>
</ul>
<p><small>[1] 神经网络理论上可以拟合任意的函数</small></p>
<p><small>[2] 也可以用于回归, 但是绝大多数的应用都在分类上, 如果在回归也需要在最后一层只有一个神经元且不给激活函数</small></p>
<h3 id="线性的ann结构">线性的ANN结构</h3>
<h4 id="基本结构">基本结构</h4>
<p>以<strong>分类问题作为切入点</strong> , 一个<strong>简单的线性ANN</strong> 的结构可以如下表示:</p>
<img src='/image/ann/linear-ann.png' style='width:300px'>
<center>single layer Linear ANN
<p>在这个结构中, 即使和我们的<strong>线性分类问题基本一致</strong> , 输入为数据的特征向量(Input layer), 输入为其线性相加:</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=Net&space;=&space;Y&space;=&space;\sum_{i=1}^nx_iw_i" target="_blank"><img src="https://latex.codecogs.com/svg.latex?Net&space;=&space;Y&space;=&space;\sum_{i=1}^nx_iw_i" title="Net = Y = \sum_{i=1}^nx_iw_i" /></a></p>
<p>这个在我们的线性判别的分类问题中, 就相当于我们的<strong>决策界</strong></p>
<p>然后我们再设置一个阈值T, 从而完成了一个简单的二分类问题的解结构 :</p>
<p>我们将 <em>X</em> 预测为 class1 if Y &gt;= T</p>
<p>我们将 <em>X</em> 预测为 class2 if Y &lt; T</p>
<h4 id="加入偏置项-bais">加入偏置项 Bais</h4>
<p>我们假设阈值 T = 0,  为了方便表述, 输入神经元的个数(即输入数据的特征个数)设置为2. 从而我们有</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=x_1w_1&space;&plus;&space;x_2w_2&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/svg.latex?x_1w_1&space;&plus;&space;x_2w_2&space;=&space;0" title="x_1w_1 + x_2w_2 = 0" /></a></p>
<p>这一条二维平面的<strong>直线形成了我们的决策界 (Decision region/boundary)</strong>, 从而在直线两边的点可以分别被分类.</p>
<p>但是有一个问题就是 : <strong>这一条决策直线是一直经过原点的</strong> , 从而给我们的分类问题造成了限制. 为了解决这个问题, 我们给这个结构加入一个偏置b, 如图 :</p>
<img src='/image/ann/linear-ann-addb.png' style='width:300px'>
<ul>
<li>加入的b[1]可以为任意常数(当为0的时候, 就默认回到过原点的状态)</li>
<li>从而我们ANN的输入层的维度增加一个默认为1的点</li>
</ul>
<p>从而我们的公式变为:</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=Net&space;=&space;Y&space;=&space;b&space;&plus;&space;\sum_{i=1}^nx_iw_i" target="_blank"><img src="https://latex.codecogs.com/svg.latex?Net&space;=&space;Y&space;=&space;b&space;&plus;&space;\sum_{i=1}^nx_iw_i" title="Net = Y = b + \sum_{i=1}^nx_iw_i" /></a></p>
<p><small>[1] 其实这里的b也可以看为非零的阈值, 但我们通常将阈值设置为0</small></p>
<h4 id="训练线性ann">训练线性ANN</h4>
<p>可以看到这个决策界就是被ANN的weight和bias决定的, 所我我们<strong>训练ANN的过程就是找到最佳的weight 和 bias的过程</strong>, 寻找最佳参数的过程我们可以利用 :</p>
<h5 id="lmsdelta-rule">LMS(delta) rule</h5>
<p>这个规则旨在利用<strong>梯度下降法(证明见本节末)<strong>找到</strong>线性神经网络</strong>的最优解. 对于模型好坏的衡量标准, 我们LMS作为我们的误差函数, 对于P个sample我们的误差函数如下:</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=E&space;=&space;\frac{1}{2}\sum_{i=1}P(t(i)&space;-&space;y(i))^2" target="_blank"><img src="https://latex.codecogs.com/svg.latex?E&space;=&space;\frac{1}{2}\sum_{i=1}P(t(i)&space;-&space;y(i))^2" title="E = \frac{1}{2}\sum_{i=1}P(t(i) - y(i))^2" /></a></p>
<p>(E is an error function of Weight)(用1/2的为了梯度下降的微分方便)</p>
<p>我们希望去Minimuize这个误差函数, 从而找到对应的 Weight 和  Bias</p>
<p>利用<strong>梯度下降法</strong>有以下步骤:</p>
<ul>
<li>
<p>对E求相对于Weight向量的各个偏导数(<strong>Partial Derivative</strong>)   :<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;E&space;=&space;(\frac{\delta&space;E}{\delta&space;w_1},&space;...&space;,&space;\frac{\delta&space;E}{\delta&space;w_n})" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\Delta&space;E&space;=&space;(\frac{\delta&space;E}{\delta&space;w_1},&space;...&space;,&space;\frac{\delta&space;E}{\delta&space;w_n})" title="\Delta E = (\frac{\delta E}{\delta w_1}, ... , \frac{\delta E}{\delta w_n})" /></a></p>
</li>
<li>
<p>更新我们的Weight向着更低的方向移动 : [1] <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_i&space;-=&space;\frac{\delta&space;E}{\delta&space;w_i}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\Delta&space;w_i&space;-=&space;\frac{\delta&space;E}{\delta&space;w_i}" title="\Delta w_i -= \frac{\delta E}{\delta w_i}" /></a>  即换一种写法, 我们可以写为 :</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;W_i&space;=&space;\alpha*x_i&space;*\delta_i" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\Delta&space;W_i&space;=&space;\alpha*x_i&space;*\delta_i" title="\Delta W_i = \alpha*x_i *\delta_i" /></a> [2]</p>
</li>
<li>
<p>Repeat 直到达到收敛条件</p>
</li>
</ul>
<p><small>[1] 经过计算, 在线性神经网络上, 这个式子可以被简化为 : <a href="https://www.codecogs.com/eqnedit.php?latex=w_i&space;&plus;=&space;\alpha&space;*&space;x_i&space;*&space;(t_i&space;-&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/svg.latex?w_i&space;&plus;=&space;\alpha&space;*&space;x_i&space;*&space;(t_i&space;-&space;y_i)" title="w_i += \alpha * x_i * (t_i - y_i)" /></a>, t, y 分别为target和预测值, xi 为第i个输入层神经元</small></p>
<p><small>[2] 这种写法将在我们的MLP后向传播过程中用到</small></p>
<h5 id="算法描述">算法描述</h5>
<img src='/image/ann/lms-algo.png'>
<p><strong>其中 alpha 为我们的学习速率,  定义每一步移动步长大小</strong></p>
<h5 id="更新策略">更新策略</h5>
<ul>
<li>Batch 更新 : 我们读取完所有的数据之后, 再去更新</li>
<li>On-line 更新 : 我们每读取一个数据, 就跟新一次</li>
</ul>
<p>更新策略对应到算法上就形成下面两种形式:</p>
<ul>
<li>Sequential mode (on-line mode) : 每读取一个数据, 跟新一次Weight 喝 Bias</li>
<li>Batch mode : 在读取完所有的数据后, 再更新我们的Weight</li>
</ul>
<p>利用 On-line 的方式会使得我们对数据的顺序更加敏感, <strong>使用Batch一般会得到更好的结果</strong></p>
<h5 id="notes">Notes:</h5>
<ul>
<li>在算法中, E 是单调减小的, 但是可能最后收敛到局部最小值(local minimum)</li>
<li>该算法运用在<strong>线性系统中 (Linear System)</strong> 即没有加激活函数</li>
</ul>
<h4 id="对于线性ann的总结">对于线性ANN的总结</h4>
<ul>
<li>
<p>线性的ANN结构不管含有多少层神经网络, 都是线性的! 证明如下 :</p>
<img src='/image/ann/Muti-linear-still-linear.png' style="width:600px">
<p>可以看到三层的ANN, 如果隐藏层是线性的函数, 那么最后的输出结果依然是x1, x2线性的组合表示, 可以推广到多层 : <strong>多层的ANN, 只要激活函数是线性的, 不管中间有多少层, 结果依然是线性的, 且可以等效为单层的ANN</strong>.</p>
</li>
<li>
<p>线性的ANN结构只能解决线性可分(<strong>Linearly separable</strong>[1])的问题</p>
</li>
</ul>
<p><small>[1] if two classes of patterns can be <strong>fully separated</strong> by the <strong>linear equation</strong>, then they are said to be linearly separable</small></p>
<h3 id="非线性ann结构">非线性ANN结构</h3>
<p>线性结构下的ANN, 虽然简单, 但是面对线性不可分(<strong>Linearly inseparable</strong>)的问题便束手无策. 以最简单的异或门举例子.</p>
<h4 id="异或xor问题导致的ai寒冬">异或XOR问题导致的AI寒冬</h4>
<p>对于线性结构下的ANN, 在空间上甚至无法区分非线性的class, 导致进入了AI寒冬</p>
<p>异或门电路的真值表如下:</p>
<table>
<thead>
<tr>
<th>x1</th>
<th>x2</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>二维空间上表示为坐标轴上的四个点. 上下一类, 左右一类. <strong>找不出任何一个线性函数可以将它们区分</strong></p>
<h5 id="非线性ann解决xor问题">非线性ANN解决XOR问题</h5>
<p>这个问题当我们<strong>非线性函数作为隐藏层的激活函数的时候</strong> , 便可以得到解决, 结构如下:</p>
<img src='/image/ann/xor.png' style='width:400px'>
<center>XOR问题
<p>我们选用了<strong>阶跃函数 (heaviside)</strong>, 作为激励函数.</p>
<p>阶跃函数定义如下 :</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">h(x) = 0 if x &lt; 0 
h(x) = 1 if x &gt;= 0
</code></pre></td></tr></table>
</div>
</div><p>从而我们可以验证 :</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">当输入(x1, x2) = (0, 0) 时 :
隐藏层左 = h(0*1 + 0*1 + -1.5) = 0
隐藏层右 = h(0*1 + 0*1 + -0.5) = 0
输出层 = h(0*-1 + 0*1 + -0.5) = 0
输出为0
</code></pre></td></tr></table>
</div>
</div><h4 id="引入mlp-multi-layer-perceptron-networks">引入MLP (Multi-layer Perceptron Networks)</h4>
<p>对于<strong>多层的</strong>具有<strong>非线性</strong>结构的ANN, 我们将<strong>它称为MLP</strong>有如下特点 :</p>
<ul>
<li>至少包含一个隐藏层(hidden layer)</li>
<li>隐藏层的神经元必须是<strong>非线性(non-linear)的激励函数</strong> , **非隐藏层(例如输出层)**可以使用线性 f(x) = x 的激励函数</li>
<li>MLP的每一层的都是和下一层<strong>全连接(fully connected)</strong>, 且每个连接都有各自的<strong>权重(Weighted)</strong>
<ul>
<li>对于一个MLP的训练算法通常叫做 <strong>Back-propagation Algorithm</strong> 是我们梯度下降(gradient descent)思想的一个应用 , 包括两个过程:</li>
<li><strong>前向传播 (Forward phase)</strong> : 权重固定(fixed), 输入向量经过网络中各个层的传递到输出层的过程</li>
<li><strong>后向传播 (Backward phase)</strong> : 通过比较输出和期望获得误差信号, 然后逐步的向后调整权值达到减小误差的目的</li>
</ul>
</li>
</ul>
<h5 id="前向传播过程-forward-phase">前向传播过程 (Forward phase)</h5>
<p>所谓的前向传播就是由<strong>输入数据的特征向量获得输出的过程</strong> , 其中 output layer 的输出结果, 就是整个MLP的输出结果. 在 MLP 中任意一个神经元的输出值计算公式如下 :</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=x_j^n&space;=&space;f(\sum_{i=0}^m&space;w_{ij}x_i^{n-1})" target="_blank"><img src="https://latex.codecogs.com/svg.latex?x_j^n&space;=&space;f(\sum_{i=0}^m&space;w_{ij}x_i^{n-1})" title="x_j^n = f(\sum_{i=0}^m w_{ij}x_i^{n-1})" /></a></p>
<ul>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=x_j^n" target="_blank"><img src="https://latex.codecogs.com/svg.latex?x_j^n" title="x_j^n" /></a> : 代表了第n层的第j个神经元</li>
<li>f : 即为该层的激励函数</li>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=w_{ij}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?w_{ij}" title="w_{ij}" /></a> : 即为连接i, j 的权重</li>
</ul>
<h5 id="后向传播过程-backward-phase">后向传播过程 (Backward phase)</h5>
<p>后向传播就是根据 <strong>预测值和真实值的误差, 逐步的由后向前纠正权重的过程</strong> , 实现的原理和线性的ANN类似, 也是<strong>利用了梯度下降法, 将权重向误差小的方向调整.</strong> 在这个模型的后向传播中, 我们默认:</p>
<ul>
<li>隐藏层的激励函数可导[1] , f(x) 求导 用 f'(x) 表示</li>
<li>输出层的激励函数为线性, f(x) = x (即没有加激励函数)</li>
</ul>
<p>接下来我们按步骤描述这一个过程[4] :</p>
<ol>
<li>
<p>首先我们要有一个 <strong>cost function[2]</strong>  e.g. 均方差:  <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{2n}&space;\sum_{i=1}^{n}&space;(h(x)_i&space;-&space;y_i))^2" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\frac{1}{2n}&space;\sum_{i=1}^{n}&space;(h(x)_i&space;-&space;y_i))^2" title="\frac{1}{2n} \sum_{i=1}^{n} (h(x)_i - y_i))^2" /></a> ,为了计算的方便, 我们可以将该目标函数变为: <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{2}&space;\sum_{i=1}^{n}&space;(h(x)_i&space;-&space;y_i))^2" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\frac{1}{2}&space;\sum_{i=1}^{n}&space;(h(x)_i&space;-&space;y_i))^2" title="\frac{1}{2} \sum_{i=1}^{n} (h(x)_i - y_i))^2" /></a>[3]</p>
<p><strong>注意这里的求和是求的n个输出层神经元的误差,  而不是n个sample</strong></p>
</li>
<li>
<p>首先从最后一层 <strong>ouput layer</strong> 开始计算. 由于输出层的激励函数就是 f(x) = x , 所以在调整<strong>最后一层记为n, 和倒数第二层记为n-1</strong> 的时候, 计算公式和线性ANN的调整方法(计算方法)一样, 可以写为:
<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_{ij}&space;=&space;\alpha&space;*&space;\delta_i&space;*&space;x_j" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\Delta&space;w_{ij}&space;=&space;\alpha&space;*&space;\delta_i&space;*&space;x_j" title="\Delta w_{ij} = \alpha * \delta_i * x_j" /></a> 参数说明如下 :</p>
<ul>
<li>xj为第n-1层的第j个元素</li>
<li>wij即为连接第n层神经元i和第n-1层神经元j的权重</li>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=\delta_i" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\delta_i" title="\delta_i" /></a> = (xi - ti)为第i个输出神经元和期待值(target)的差</li>
<li>alpha为学习速率.</li>
</ul>
<p>上述过程反应到如图, 可以得到 : <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_{46}&space;=&space;\alpha&space;f'_6(e)\delta&space;x_4&space;=&space;\alpha\delta&space;x_4" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\Delta&space;w_{46}&space;=&space;\alpha&space;f'_6(e)\delta&space;x_4&space;=&space;\alpha\delta&space;x_4" title="\Delta w_{46} = \alpha f'_6(e)\delta x_4 = \alpha\delta x_4" /></a></p>
<p><img src='/image/ann/ann-back-1.png' style='width:500px' /></p>
<p><strong>在计算出这个变化量后, 不应该马上调整, 因为后续的调整步骤依赖于这个原始的权值, 所以需要计算出所有的权值之后, 再进行更新</strong></p>
</li>
<li>
<p>计算完输出层后, 我们开始计算隐藏层的权重更新. 利用一张图示可以清晰的描述出反向传播, 图中可以看出<strong>反向传播就是以误差为输入, 方向由输出到输入的传播过程</strong> , 我们在输出层计算出误差以后, 就可以将误差向输出层传播调整权值. 此时的理解就是输出当输入, 传播反向. 从而可以得到n-1层的误差. 有 :</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_{14}&space;=&space;\alpha&space;f'_4(e)\delta_4&space;x_1" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\Delta&space;w_{14}&space;=&space;\alpha&space;f'_4(e)\delta_4&space;x_1" title="\Delta w_{14} = \alpha f'_4(e)\delta_4 x_1" /></a> (其中的函数即为隐藏层激励函数的导数了, 其中e是前向传播过程这个神经元的sum项)</p>
<img src='/image/ann/back-phase.png' style='width:500px'>
</li>
</ol>
<p><small>[1] 由于利用了梯度下降法, 所以后向传播的过程要求了我们的<strong>激励函数是可导</strong>的</small></p>
<p><small>[2] 用来衡量我们模型的误差, 这也是我们要减小的目标函数</small></p>
<p><small>[3] 注意这里的求和是求的n个输出层神经元的误差,  而不是n个sample</small></p>
<p><small>[4] <a href="https://www.jianshu.com/p/3a65213e68a8">反向传播过程</a> </small></p>
<h5 id="公式总解">公式总解</h5>
<p><strong>所以我们的公式总计可以总结如下:</strong></p>
<p>- 对于forward-phase:</p>
<img src='/image/ann/forward-eq.png'>
<p>- 对于backward-phase:</p>
<img src='/image/ann/backward-eq.png'/>
<ul>
<li>输出层来说, 激励函数一般为线性的, 所以其导数一般为1</li>
<li>此时隐藏层的激励函数的导数的自变量 Vj(n) 即为前向传播过程中, 这一步的值</li>
</ul>
<h5 id="算法描述-1">算法描述</h5>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">The MLP training algorithm :
1: init the weights in the MLP with random values
2: Repeat 
3: 	 for each sample x in the training set
4:		y0 = MLP_output(x0);
5:		e  = calculate error (target - y0)
6: 		compute the delta_W for all weights by using the gradient descent 
7: 		update the Weights
8:	  Until MLP error converge
</code></pre></td></tr></table>
</div>
</div><h4 id="激励函数种类">激励函数种类</h4>
<blockquote>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247483977&amp;idx=1&amp;sn=401b211bf72bc70f733d6ac90f7352cc&amp;chksm=fdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3&amp;scene=21#wechat_redirect">理解激活函数</a></p>
</blockquote>
<p>这些激活函数保证了MLP的非线性映射, 只要激活函数选的合理, 理论上, 三层MLP就可以实现对任意函数的逼近 (<strong>万能逼近(Universal Approximator)定理</strong>)</p>
<h5 id="sigmiod-函数">Sigmiod 函数</h5>
<p>公式为:  <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{1&plus;e^{-x}}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\frac{1}{1&plus;e^{-x}}" title="\frac{1}{1+e^{-x}}" /></a></p>
<p>函数图像为 :</p>
<p>无穷小点为0, 无穷大点为1</p>
<img src='/image/ann/sigmoid.png' style="width:400px">
<h5 id="tanh-函数">tanh 函数</h5>
<p>公式为 : <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1-e^{-2x}}{1&plus;e^{-2x}}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\frac{1-e^{-2x}}{1&plus;e^{-2x}}" title="\frac{1-e^{-2x}}{1+e^{-2x}}" /></a></p>
<p>图像为 :</p>
<img src='/image/ann/tanh.png' style="width:400px">
<h5 id="softmax-函数">Softmax 函数</h5>
<p>公式为 :  <a href="https://www.codecogs.com/eqnedit.php?latex=y_k&space;=&space;\frac{e^{v_k}}{\sum_{k-1}^Ke^{v_k}}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?y_k&space;=&space;\frac{e^{v_k}}{\sum_{k-1}^Ke^{v_k}}" title="y_k = \frac{e^{v_k}}{\sum_{k-1}^Ke^{v_k}}" /></a></p>
<p>可以将不同的输入映射到(0,1)区间, 且值可以看为输入值的概率分布 :</p>
<img src='/image/ann/softmax.png' style='width:500px'/>
<h4 id="对于非线性annmlp的总结">对于非线性ANN(MLP)的总结</h4>
<ul>
<li>MLP 相对于我们来说就是一个黑盒子‘black box’. 特别是在deep learning领域, 我们不知道为什么会有这样的结果, 但是就是发生了. (There is no rule to be extracted)</li>
</ul>
<h5 id="limitation">Limitation</h5>
<ul>
<li>这是一个 <strong>Asymptotic algorithm</strong> : 意味着误差会 <strong>缓慢的</strong> 接近最小.</li>
<li>很容易收敛到局部最小值</li>
<li>有很多参数需要设置, 所以在<strong>样本空间小</strong>的情况下, <strong>很容易导致过拟合</strong>, 这也是近几年在拥有大量的数据量和计算能录后, Deep learning 才火起来的原因.</li>
</ul>
<h5 id="小tips">小Tips</h5>
<ul>
<li>学习速率通常是0,1之间的数, 可以动态改变, 且更有利于收敛</li>
<li>输出神经元一般利用线性的激励函数因为: 有些非线性的激励函数的值域有限制, e.g. Sigmoid函数值域为[0, 1]这和我们的样本的target值会相差较大</li>
</ul>
<h4 id="非线性annmlp的知识点问答">非线性ANN(MLP)的知识点问答</h4>
<ul>
<li>
<p><strong>Q : 我们增加MLP的隐藏层会有什么影响 ?</strong></p>
<p>A : 1. 增加隐藏层可以模拟复杂性增加的问题, 但是也会导致<strong>梯度消失的问题</strong>. 2. 更为明显的就是可能造成overfitting.</p>
</li>
<li>
<p><strong>Q : 基于万能逼近定理, 我们三层MLP可以逼近任意函数, 是不是意味着, 我们不需要高于三层的MLP模型?</strong></p>
<p>不是, 有些复杂的模型, 在三层MLP拟合下, 会变的特别复杂, 虽然可以做到, 但是代价太大.比如, 对于一个简单的线性可分的问题, 我们不可能选择很复杂的模型, 虽然复杂的模型可以拟合出来.</p>
</li>
</ul>
<h3 id="非线性annmlp的训练">非线性ANN(MLP)的训练</h3>
<p><strong>目标</strong> : 训练出<strong>泛化能力强(generalization)</strong>, 准确率高的MLP模型.</p>
<p>所谓的泛化能力指的就是模型在<strong>新数据前预测准确率依然很高的能力</strong>. 泛化能力强的模型, 一定不是过拟合(Overfitting)的. 下图是泛化误差和训练误差的图表 :</p>
<img src='/image/ann/training-error.png' style='width:600px'>
<ul>
<li>可以看到, 我们的训练次数增多, 泛化误差会越来越大, 从而导致过拟合的问题出现</li>
<li>我们希望训练出来的MLP停止在这个<strong>泛化误差的最低点, 而不是训练误差的最底点</strong></li>
</ul>
<h4 id="所有的训练数据拿来训练模型-all-in-训练法">所有的训练数据拿来训练模型 (All in 训练法)</h4>
<p>这种简单的方法就将<strong>所有的数据都拿出来训练</strong> (显然不是一个很好的方法) , 这种方法很容易导致下面的问题</p>
<ul>
<li>模型很容易导致overfit, 特别是在输入的数据特征值特别多的情况下</li>
<li>虽然训练的误差值很小, 但是泛化误差很大</li>
</ul>
<h4 id="hold-out-method-三分训练法">Hold-out method (三分训练法)</h4>
<p>这种方法的思想就是, 我们将训练数据集分为三部分 : <strong>training dataset, validating dataset, test dataset</strong>, 一般的占比为 3 : 1 :1</p>
<ul>
<li>Training dataset : 用于获得MLP模型的基本参数如 Weight, bias. 就是训练时候用到的数据集</li>
<li>Validation dataset : 用于阶段性的检验MLP的泛化能力, 从<strong>而可以让我们人为的调整超参数[1].</strong></li>
<li>Test dataset : 用于评估模型的泛化能力.</li>
</ul>
<h5 id="验证集-和-测试集的区别">验证集 和 测试集的区别</h5>
<table>
<thead>
<tr>
<th>类别</th>
<th>验证集</th>
<th>测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td>是否训练模型时用到</td>
<td>否</td>
<td>否</td>
</tr>
<tr>
<td>作用</td>
<td>用于调超参数</td>
<td>验证评估泛化能力</td>
</tr>
<tr>
<td>使用此时</td>
<td>多次使用, 不断调参</td>
<td>使用一次</td>
</tr>
</tbody>
</table>
<h5 id="tips">Tips;</h5>
<ul>
<li>及早的停止学习可以<strong>减小训练误差</strong> 和 <strong>验证误差</strong></li>
</ul>
<p><small>[1] 就是模型之外的全局参数比如 : 学习速率等等</small></p>
<h4 id="交叉验证-n-fold-cross-validation">交叉验证 (N-fold) Cross-validation</h4>
<p>这种是更加常用的方法. 特别是当数据量小的情况下. 这是一个很好的验证方法</p>
<h4 id="后向传播的提升方法">后向传播的提升方法</h4>
<p>对于特别复杂的MLP, 后向传播算法会变的非常<strong>time expensive</strong>, 我们可以用下面的方法去提升后向传播的速度</p>
<ul>
<li>GPU !!!!</li>
<li>数据预处理阶段减小输入数据的维度</li>
<li>归一化数据(Normalize)</li>
<li>用动态的学习速率</li>
<li>&hellip;&hellip;</li>
</ul>
<p>**用基于算法的形式去改进 : **</p>
<h5 id="1-基于动量momentum的后向传播改进算法">1. 基于动量(Momentum)的后向传播改进算法</h5>
<p>可以在不减少稳定性的条件下, 提高MLP的收敛速度. 如下可以在我们的权重改变公式下,加入一个额外的项.</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\Delta_{ij}(t)&space;=&space;\eta&space;\delta_j(t)y_i(t)&space;&plus;&space;\alpha[w_{ij}(t)&space;-&space;w_{ij}(t-1)]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta_{ij}(t)&space;=&space;\eta&space;\delta_j(t)y_i(t)&space;&plus;&space;\alpha[w_{ij}(t)&space;-&space;w_{ij}(t-1)]" title="\Delta_{ij}(t) = \eta \delta_j(t)y_i(t) + \alpha[w_{ij}(t) - w_{ij}(t-1)]" /></a></p>
<p>其中 alpha 是一个常数</p>
<p>加入动量的效果:</p>
<ul>
<li>如果变化方向相同 : 则动量加大改变幅度, 加大收敛速度</li>
<li>如果变化方向相反 : 动量减小摆动的幅度, 增加稳定性. 一定程度上可以防止被困于局部值</li>
</ul>
<h5 id="2stochastic-gradient-decent随机梯度下降">2.Stochastic gradient decent(随机梯度下降)</h5>
<h3 id="一些关键证明">一些关键证明</h3>
<h4 id="证明梯度下降法gradient-based的可行性">证明梯度下降法(gradient-based)的可行性:</h4>
<p>假设我们需要优化的函数为的f(x), 那么在点x的导数为f'(x), 这个导数给予我们函数的在点x的倾斜度(Slope), 根据泰勒展开公式我们有 :</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=f(x&space;&plus;&space;\varepsilon&space;)&space;\approx&space;f(x)&space;&plus;&space;\varepsilon'f(x)&space;\quad&space;s.t.&space;\varepsilon&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/svg.latex?f(x&space;&plus;&space;\varepsilon&space;)&space;\approx&space;f(x)&space;&plus;&space;\varepsilon'f(x)&space;\quad&space;s.t.&space;\varepsilon&space;>&space;0" title="f(x + \varepsilon ) \approx f(x) + \varepsilon'f(x) \quad s.t. \varepsilon > 0" /></a></p>
<p>即说明有公式 : f(x - $\epsilon$f'(x))  &lt; f(x)</p>
<p>通常 $\epsilon$ 可以看作我们做梯度下降时候的步长</p>
<p>假设在二维的情况下 : 所以可以看出, 当导数大于0的时候, 我们的x应该向左移动(即x轴的负方向).</p>
<p><strong>所以此时我们x应该移动的方向是: x -= $\epsilon$f'(x)</strong></p>
<p>所以有了如下结论:</p>
<p>So we reduce f (x) by moving x in small steps with the <strong>opposite sign</strong> of the derivative</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">LYR</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2021-08-14
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      
      <nav class="post-nav">
        
        <a class="next" href="/post/10.%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/svm/">
            <span class="next-text nav-default">SVM 向量机</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:lyr-2000@qq.com" class="iconfont icon-email" title="email"></a>
  <a href="http://doc.lyr-2000.xyz/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span>lyr</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>






<script src="/js//custom/latex_plugin.js"></script>


</body>
</html>
